{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"d_12_sentiment_gap.ipyny","private_outputs":true,"provenance":[{"file_id":"1FcWCn8mHCF79nPzRUyei6i9L-coprJsh","timestamp":1617861491297}],"authorship_tag":"ABX9TyMc21Pss7tS4szhr8DwXz83"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"o_jFVqKEJDLY"},"source":["import tensorflow as tf\n","\n","dataset = tf.keras.utils.get_file(\n","    fname=\"aclImdb.tar.gz\", \n","    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","    extract=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xW4E7Q7lJiKc"},"source":["# unsup = unsupervised (非監督式)[沒有給答案的 ex:分群]\n","# 深度學習必給答案\n","# with open () as ... 一定會關閉檔案 不須加 close\n","# textrap() 美觀用(每幾個字做縮牌動作)\n","import glob\n","import os\n","dn = os.path.dirname(dataset)\n","fn = glob.glob(dn + \"/aclImdb/train/pos/*\")\n","with open(fn[0], \"r\", encoding=\"utf-8\") as f:\n","  print(\"pos:\",f.read())\n","  fn = glob.glob(dn + \"/aclImdb/train/neg/*\")\n","with open(fn[0], \"r\", encoding=\"utf-8\") as f:\n","  print(\"neg:\",f.read())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRoMtvjoNQda"},"source":["import pandas as pd\n","def getdata(base):\n","  datas = {\"article\":[], \"ans\": []}\n","  targets = os.path.join(base, \"pos\", \"*\")\n","  for fn in glob.glob(targets):\n","    with open(fn, \"r\", encoding=\"utf-8\") as f:\n","      datas[\"article\"].append(f.read())\n","      datas[\"ans\"].append(1)\n","  targets = os.path.join(base, \"neg\", \"*\")\n","  for fn in glob.glob(targets):\n","    with open(fn, \"r\", encoding=\"utf-8\") as f:\n","      datas[\"article\"].append(f.read())\n","      datas[\"ans\"].append(0)\n","  return pd.DataFrame(datas)\n","\n","train = os.path.join(dn, \"aclImdb\", \"train\")\n","train_df = getdata(train)\n","test = os.path.join(dn, \"aclImdb\", \"test\")  \n","test_df = getdata(test)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGJEuylvZw75"},"source":["# Step 1 -> Tokenize: 1:I , 2:Love , 3:you\n","# num_word:最常出現的單字, filters:標點符號(中文全形標點符號要去掉), char_level:以字來做切割(ex: apple-> a,p,p,l,e) \n","# 0不要用，0是用來做padding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","tok = Tokenizer(num_words=4000)\n","tok.fit_on_texts(train_df[\"article\"])\n","x_train_seq = tok.texts_to_sequences(train_df[\"article\"])\n","x_test_seq = tok.texts_to_sequences(test_df[\"article\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40lpOhm_dgxN"},"source":["# 為什麼會有NaN,最常那邊有多少就會有多少格,沒有的文章那格就會出現NaN\n","pd.DataFrame(x_train_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1M8kYsr6dPSr"},"source":["# 查詢index的單字\n","tok.index_word[353]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DDDjcHTepda"},"source":["# 有東西消失是因過最常出現3000單字\n","# \" \".join([tok.index_word[n] for n in x_train_seq[0]])\n","tok.sequences_to_texts(x_train_seq)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UrTYOnVCnSH"},"source":["# 512 & 1024 差不多\n","MAXLEN = 512"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZuzokHQf4wB"},"source":["# pad_sequences(): padding:pre[補在前面], truncating:pre[刪除前面], maxlen:[文章最大值]\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","x_train_pad = pad_sequences(x_train_seq, maxlen=MAXLEN)\n","x_test_pad = pad_sequences(x_test_seq, maxlen=MAXLEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJdK7HFZa3n3"},"source":["# embedding 嵌入層 -> 將正整數轉換為一個向量 \n","# embeddind() -> imput_dim[詞彙數量](num_word + 1[padding]), mask_zero(零進去後不會處裡/記得設定)\n","# embedding.param 3001*128\n","from tensorflow.keras.layers import Flatten, Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n","layers =[\n","  Embedding(4001, 128, mask_zero=True, input_length=MAXLEN),\n","  GlobalAveragePooling1D(),\n","  Dense(2, activation=\"softmax\")\n","]\n","model = Sequential(layers)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBdLoZ3GBf7Q"},"source":["from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","model.compile(loss=SparseCategoricalCrossentropy(),\n","       optimizer=\"adam\",\n","       metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQmYWisOBrao"},"source":["import numpy as np\n","y_train = np.array(train_df[\"ans\"])\n","y_test = np.array(test_df[\"ans\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9-iznjJBlIt"},"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","callbacks = [\n","   EarlyStopping(patience=5, restore_best_weights=True),\n","   ModelCheckpoint(\"sentiment.h5\", save_best_only=True)\n","]\n","model.fit(x_train_pad,\n","     y_train,\n","     batch_size=200,\n","     epochs=50,\n","     validation_split=0.1,\n","     verbose=2,\n","     callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASqa5fRuCRKx"},"source":["model.evaluate(x_test_pad, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XR_nYluqDXJl"},"source":["from tensorflow.keras.layers import Flatten, Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n","layers =[\n","  Embedding(4001, 128, mask_zero=True),\n","  GlobalAveragePooling1D()\n","]\n","temp = Sequential(layers)\n","temp.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wovHODbrEMiF"},"source":["# get_weight()取出 set_weight()設置回\n","w = model.layers[0].get_weights()\n","temp.layers[0].set_weights(w)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-creP4KEobr"},"source":["# 這邊的香度是關於這個主題中的貢獻兩比較的相似度\n","from numpy import dot\n","from numpy.linalg import norm\n","w1 = tok.word_index[\"movie\"]\n","pre = temp.predict([[w1]])\n","e1 = pre[0]\n","print(w1, e1)\n","w2 = tok.word_index[\"cinema\"]\n","pre = temp.predict([[w2]])\n","e2 = pre[0]\n","print(w1, w2)\n","cos_sim = dot(e1, e2)/(norm(e1)*norm(e2))\n","print(\"相似度\", cos_sim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFm2rQf8KAUd"},"source":["s1 = \"this movie is interesting\"\n","w1 = [tok.word_index[w] for w in s1.split(\" \")]\n","pre = temp.predict([w1])\n","e1 = pre[0]\n","s2 = \"this movie is boring\"\n","w2 = [tok.word_index[w] for w in s2.split(\" \")]\n","pre = temp.predict([w2])\n","e2 = pre[0]\n","print(w1, w2)\n","cos_sim = dot(e1, e2)/(norm(e1)*norm(e2))\n","print(\"相似度\", cos_sim)"],"execution_count":null,"outputs":[]}]}